{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd0d832",
   "metadata": {},
   "source": [
    "# SI140@fall2021 project\n",
    "\n",
    "## metadata\n",
    "\n",
    "- abstract: performance evalutaion of classical MAB algorithms\n",
    "- DUE date: 2022/01/16 11:59am\n",
    "- author: spinach/hehelego (彭程 pengcheng2 2020533068)\n",
    "\n",
    "## environment\n",
    "\n",
    "- OS: arch linux\n",
    "- kernel: 5.15.7-arch1-1\n",
    "- Arch: x86-64 (amd64)\n",
    "- python: version 3.9.9\n",
    "- ipython: version 7.30.1\n",
    "- ipython-kernel: 6.6.0\n",
    "- numpy: version 1.21.3\n",
    "- matplotlib: version 3.5.0\n",
    "\n",
    "## table of contents\n",
    "\n",
    "- MAB setup\n",
    "- implementation of the three appointed classical bandit learning algorithms\n",
    "- implementation of several more algorithms\n",
    "- simulation\n",
    "- performance analysis\n",
    "\n",
    "The above sections can be found in this jupyter notebook,\n",
    "while the discussion section can be found in `report.pdf`.\n",
    "\n",
    "## code style\n",
    "\n",
    "- multiple statement in one line is now allowed\n",
    "- identifier for variable      `snake_case`\n",
    "- identifier for constant      `SANKE_CASE`\n",
    "- identifier for function      `snake_case`\n",
    "- identifier for class         `CamelCase`\n",
    "- identifier for method        `snake_case`\n",
    "- identifier for property      `snake_case`\n",
    "- identifier for class method  `snake_case`\n",
    "- identifier for static method `snake_case`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6444b4",
   "metadata": {},
   "source": [
    "### Sampling from $\\mathrm{Beta}(a,b)$ with integer paramters\n",
    "\n",
    "Suppose that $X\\sim \\mathrm{Gamma}(a,\\lambda), Y\\sim \\mathrm{Gamma}(b,\\lambda)$ be independent,\n",
    "let $T=X+Y,W=\\frac{X}{X+Y}$.  \n",
    "By _bank post-office_ story, we have $T\\sim\\mathrm{Gamma}(a+b,\\lambda), W\\sim \\mathrm{Beta}(a,b)$  \n",
    "After some testing, we found that bigger $\\lambda$ is better.\n",
    "\n",
    "Alternatively, we can use the $a$-th ordered statistics of $a+b-1$ i.i.d. $\\mathrm{Unif}(0,1)$ r.v.s.  \n",
    "\n",
    "However, both methods need generating $a+b$ uniforms, which can be the bottle neck of our program.  \n",
    "We found a classic paper _[Computer methods for sampling from gamma, beta, poisson and bionomial distributions\n",
    "](https://link.springer.com/article/10.1007/BF02293108)_ which covers beta-distributed random variable generation.  \n",
    "\n",
    "***TODO*** implement the beta-distribution generating method from the paper\n",
    "\n",
    "### Sampling from $\\mathrm{Gamma}(n,\\lambda)$ distribution\n",
    "\n",
    "Let $X_1,X_2\\ldots X_n$ be $n$ i.i.d. $\\mathrm{Expo}(\\lambda)$ r.v.s,\n",
    "then their sum $S=\\sum_{i=1}^n X_i$ has a $\\mathrm{Gamma}(n,\\lambda)$ distribution.\n",
    "\n",
    "### Generating $\\mathrm{Expo}(\\lambda)$ from $\\mathrm{Unif}(0,1)$\n",
    "\n",
    "The CDF of $\\mathrm{Expo}(\\lambda)$ is $F(x)=1-e^{-\\lambda x}, F^{-1}(x) = -\\frac{\\ln (1-x)}{\\lambda}$.  \n",
    "Let $U\\sim\\mathrm{Unif}(0,1)$ then $X=F^{-1}(U)$ has a $\\mathrm{Expo}(\\lambda)$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc6406",
   "metadata": {},
   "source": [
    "## section: MAB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ddb960",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import numpy.typing as npt\n",
    "import abc\n",
    "import typing\n",
    "\n",
    "# the (pesudo) random number generator\n",
    "rng = npr.Generator(npr.MT19937(19260817))\n",
    "\n",
    "\n",
    "def natural_number_stream(start: int = 0) -> typing.Iterator[int]:\n",
    "    r'''\n",
    "    generate a natural number stream [start, start+1, start+2, ...]\n",
    "    '''\n",
    "    while True:\n",
    "        yield start\n",
    "        start += 1\n",
    "\n",
    "\n",
    "def argmax(xs: typing.Iterable) -> int:\n",
    "    r'''\n",
    "    index of the maximum element\n",
    "    '''\n",
    "    return max(zip(natural_number_stream(), xs), key=lambda kv: kv[1])[0]\n",
    "\n",
    "\n",
    "def argmin(xs: typing.Iterable) -> int:\n",
    "    r'''\n",
    "    index of the minimum element\n",
    "    '''\n",
    "    return max(zip(natural_number_stream(), xs), key=lambda kv: -kv[1])[0]\n",
    "\n",
    "\n",
    "class Sampling:\n",
    "    r'''\n",
    "    sampling named distribution\n",
    "\n",
    "    methods for single sample generation:\n",
    "    - uniform(a,b):        $\\mathrm{Unif}(a,b)$\n",
    "    - bernoulli(p):        $\\mathrm{Bern}(p)$\n",
    "    - exponential(rate):   $\\mathrm{Expo}(\\lambda)$, where rate is the $\\lambda$ parameter.\n",
    "    - gamma(n,rate):       $\\mathrm{Gamma}(a,\\lambda)$, where $a$ is $n$ and $\\lambda$ is rate.\n",
    "\n",
    "    methods for multiple i.i.d. samples generation:\n",
    "    - uniform_array\n",
    "    - bernoulli_array\n",
    "    - exponential_array\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(a: float = 0, b: float = 1) -> float:\n",
    "        return a+(b-a)*rng.random()\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform_array(size: int, a: float = 0, b: float = 1) -> npt.NDArray[np.float_]:\n",
    "        return a+(b-a)*rng.random(size)\n",
    "\n",
    "    @staticmethod\n",
    "    def bernoulli(prob: float) -> bool:\n",
    "        return Sampling.uniform() < prob\n",
    "\n",
    "    @staticmethod\n",
    "    def bernoulli_array(size: int, prob: float) -> npt.NDArray[np.bool_]:\n",
    "        return Sampling.uniform_array(size) < prob\n",
    "\n",
    "    @staticmethod\n",
    "    def exponential(rate: float = 1) -> float:\n",
    "        return -np.log(1-Sampling.uniform())/rate\n",
    "\n",
    "    @staticmethod\n",
    "    def exponential_array(size: int, rate: float = 1) -> npt.NDArray[np.float_]:\n",
    "        return -np.log(1-Sampling.uniform_array(size))/rate\n",
    "\n",
    "    @staticmethod\n",
    "    def gamma(n: int, rate: float = 1) -> float:\n",
    "        return Sampling.exponential_array(n, rate).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def beta(a: int, b: int, _rate: float = 1000) -> float:\n",
    "        x, y = Sampling.gamma(a, _rate), Sampling.gamma(b, _rate)\n",
    "        return x/(x+y)\n",
    "\n",
    "\n",
    "class MAB:\n",
    "    r'''\n",
    "    the multi-armed bandit\n",
    "    '''\n",
    "\n",
    "    def __init__(self, theta: list[float]):\n",
    "        self.theta = theta[:]\n",
    "        self.arms = len(theta)\n",
    "\n",
    "    def pull(self, i: int) -> int:\n",
    "        return int(Sampling.bernoulli(self.theta[i]))\n",
    "\n",
    "    def oracle_value(self, n: int) -> float:\n",
    "        return n*max(self.theta)\n",
    "\n",
    "\n",
    "class Strategy(abc.ABC):\n",
    "    r'''\n",
    "    the abstract base class for bandit algorithms\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mab: MAB, n: int):\n",
    "        self.mab = mab\n",
    "        self.n = n\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def run(self) -> int:\n",
    "        '''\n",
    "        perform one simulation: n pulls\n",
    "        '''\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def profile(self) -> str:\n",
    "        '''\n",
    "        return the strategy/algorithm name and value of parameters\n",
    "        '''\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3af02",
   "metadata": {},
   "source": [
    "## section: implementation of classical bandit learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333185de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Strategy):\n",
    "    def __init__(self, mab: MAB, n: int, eps: float):\n",
    "        super().__init__(mab, n)\n",
    "        self._profile = f'EpsilonGreedy(epsilon={eps})'\n",
    "        self.eps = eps\n",
    "        self.count = [int(0) for _ in range(mab.arms)]\n",
    "        self.theta_hat = [float(0) for _ in range(mab.arms)]\n",
    "\n",
    "    def run(self) -> int:\n",
    "        earn = 0\n",
    "        for _ in range(self.n):\n",
    "            arm = argmax(self.count)\n",
    "            if Sampling.bernoulli(self.eps):\n",
    "                arm = rng.integers(self.mab.arms)\n",
    "            reward = self.mab.pull(arm)\n",
    "            earn += reward\n",
    "\n",
    "            self.count[arm] += 1\n",
    "            self.theta_hat[arm] += (reward-self.theta_hat[arm])/self.count[arm]\n",
    "\n",
    "        return earn\n",
    "\n",
    "    @property\n",
    "    def profile(self) -> str:\n",
    "        return self._profile\n",
    "\n",
    "\n",
    "class UpperConfidenceBound(Strategy):\n",
    "    def __init__(self, mab: MAB, n: int, c: float):\n",
    "        super().__init__(mab, n)\n",
    "        self._profile = f'UpperConfidenceBound(c={c})'\n",
    "        self.c = c\n",
    "        self.count = [int(0) for _ in range(mab.arms)]\n",
    "        self.theta_hat = [float(0) for _ in range(mab.arms)]\n",
    "\n",
    "    def run(self) -> int:\n",
    "        from math import log, sqrt\n",
    "        earn = 0\n",
    "        for t in range(self.mab.arms):\n",
    "            reward = self.mab.pull(t)\n",
    "            earn += reward\n",
    "\n",
    "            self.count[t] = 1\n",
    "            self.theta_hat[t] = reward\n",
    "\n",
    "        for t in range(self.mab.arms, self.n):\n",
    "            arm = argmax(self.theta_hat[i] + self.c*sqrt(2*log(t+1)/self.count[i])\n",
    "                         for i in range(self.mab.arms))\n",
    "            reward = self.mab.pull(arm)\n",
    "            earn += reward\n",
    "\n",
    "            self.count[arm] += 1\n",
    "            self.theta_hat[arm] += (reward-self.theta_hat[arm])/self.count[arm]\n",
    "\n",
    "        return earn\n",
    "\n",
    "    @property\n",
    "    def profile(self) -> str:\n",
    "        return self._profile\n",
    "\n",
    "\n",
    "class ThompsonSampling(Strategy):\n",
    "    def __init__(self, mab: MAB, n: int, prior: list[tuple[int, int]]):\n",
    "        super().__init__(mab, n)\n",
    "        self.beta_parameters = [list(i) for i in prior]\n",
    "        self._profile = f'ThompsonSampling({prior})'\n",
    "\n",
    "    def run(self) -> int:\n",
    "        earn = 0\n",
    "        for _ in range(self.n):\n",
    "            theta_hat = [Sampling.beta(a, b)\n",
    "                         for (a, b) in self.beta_parameters]\n",
    "            arm = argmax(theta_hat)\n",
    "            reward = self.mab.pull(arm)\n",
    "            earn += reward\n",
    "\n",
    "            self.beta_parameters[arm][0] += reward\n",
    "            self.beta_parameters[arm][1] += 1-reward\n",
    "\n",
    "        return earn\n",
    "\n",
    "    @property\n",
    "    def profile(self) -> str:\n",
    "        return self._profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3bded",
   "metadata": {},
   "source": [
    "## section: several more algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7032e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonDecreaseGreedy(Strategy):\n",
    "    def __init__(self, mab: MAB, n: int, eps: float, shrink_factor: float):\n",
    "        super().__init__(mab, n)\n",
    "        self._profile = f'EpsilonDecreaseGreedy(epsilon={eps}, shrink_factor={shrink_factor})'\n",
    "        self.eps = eps\n",
    "        self.shrink_factor = shrink_factor\n",
    "        self.count = [int(0) for _ in range(mab.arms)]\n",
    "        self.theta_hat = [float(0) for _ in range(mab.arms)]\n",
    "\n",
    "    def run(self) -> int:\n",
    "        earn = 0\n",
    "        for _ in range(self.n):\n",
    "            arm = argmax(self.count)\n",
    "            if Sampling.bernoulli(self.eps):\n",
    "                arm = rng.integers(self.mab.arms)\n",
    "            reward = self.mab.pull(arm)\n",
    "            earn += reward\n",
    "\n",
    "            self.count[arm] += 1\n",
    "            self.theta_hat[arm] += (reward-self.theta_hat[arm])/self.count[arm]\n",
    "\n",
    "            self.eps *= self.shrink_factor\n",
    "\n",
    "        return earn\n",
    "\n",
    "    @property\n",
    "    def profile(self) -> str:\n",
    "        return self._profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55db579",
   "metadata": {},
   "source": [
    "## section: simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33eabeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 200\n",
    "N = 6000\n",
    "mab = MAB([0.8, 0.6, 0.5])\n",
    "\n",
    "\n",
    "def once() -> dict[str, int]:\n",
    "    eps_gre: list[Strategy] = [\n",
    "        EpsilonGreedy(mab, N, 0.2),\n",
    "        EpsilonGreedy(mab, N, 0.4),\n",
    "        EpsilonGreedy(mab, N, 0.6),\n",
    "        EpsilonGreedy(mab, N, 0.8),\n",
    "    ]\n",
    "    eps_dec_gre: list[Strategy] = [\n",
    "        EpsilonDecreaseGreedy(mab, N, 1, 0.95),\n",
    "        EpsilonDecreaseGreedy(mab, N, 0.5, 0.95),\n",
    "        EpsilonDecreaseGreedy(mab, N, 0.2, 0.99),\n",
    "    ]\n",
    "    ucb: list[Strategy] = [\n",
    "        UpperConfidenceBound(mab, N, 1),\n",
    "        UpperConfidenceBound(mab, N, 2),\n",
    "        UpperConfidenceBound(mab, N, 6),\n",
    "        UpperConfidenceBound(mab, N, 9),\n",
    "    ]\n",
    "    ts: list[Strategy] = [\n",
    "        ThompsonSampling(mab, N, [(1, 1), (1, 1), (1, 1)]),\n",
    "        ThompsonSampling(mab, N, [(601, 401), (401, 601), (2, 3)]),\n",
    "        ThompsonSampling(mab, N, [(8, 2), (6, 4), (5, 5)]),\n",
    "        ThompsonSampling(mab, N, [(80, 20), (6, 4), (5, 5)]),\n",
    "    ]\n",
    "    all_in_one = eps_gre + eps_dec_gre + ucb + ts\n",
    "\n",
    "    return {\n",
    "        s.profile: s.run()\n",
    "        for s in all_in_one\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(xs):\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "\n",
    "tmp=once()\n",
    "rec: dict[str,list[int]] = {k:[v] for (k,v) in tmp.items()}\n",
    "for _ in range(RUNS):\n",
    "    out = once()\n",
    "    for (k,v) in tmp.items():\n",
    "        rec[k].append(v)\n",
    "\n",
    "print(f'orcale {mab.oracle_value(N)}')\n",
    "for (k,v) in rec.items():\n",
    "    print(f'{k}\\t{v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b65270",
   "metadata": {},
   "source": [
    "## section: testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a976c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = rng.random(100)\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        y = rng.random(100)\n",
    "        ax[i][j].plot(x, y, 'r^-')\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
